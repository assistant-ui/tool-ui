import { DocsHeader } from "../_components/docs-header";
import { Alert, AlertDescription, AlertTitle } from "@/components/ui/alert";
import { FlaskConical } from "lucide-react";

<DocsHeader title="Overview" mdxPath="app/docs/overview/content.mdx" />

<Alert className="squircle my-6 flex max-w-2xl flex-col gap-3 rounded-4xl border-emerald-950 bg-emerald-950/20 p-5">
  <AlertTitle className="mb-0! flex items-center gap-2 pb-0! font-mono text-emerald-300">
    <FlaskConical className="size-4 text-emerald-300!" />
    {"Research Preview"}
  </AlertTitle>
  <AlertDescription className="my-0! py-0! text-pretty text-emerald-100">
    {
      "Tool UI is in active development, and APIs are still coalescing as we refine discover better patterns for conversation-native UIs."
    }
  </AlertDescription>
</Alert>

Tool UI is a React component framework for **conversation‑native** UIs inside AI chats.  
Tools return **JSON**; Tool UI renders it as **inline, narrated, referenceable** surfaces.

## At a glance

- **Conversation‑native components** that live _inside messages_, optimized for chat width and scroll.
- **Schema‑first rendering**: every surface is driven by a serializable schema with stable IDs.
- **Assistant‑anchored**: the assistant introduces, interprets, and closes each surface.
- **Stack‑agnostic**: works with any LLM/provider and orchestration layer.

## Where it fits

Radix/shadcn (primitives) → **Tool UI** (conversation‑native components & schema) → AI SDK / LangGraph / etc. (LLM orchestration)

## Who it’s for

- **React devs & design engineers** building LLM chat apps.
- Teams who want **predictable, accessible** UI that the assistant can **reference** (“the second row”).
- Anyone who wants **typed, schema‑validated** tool outputs instead of brittle HTML strings.

## Why Tool UI

- **Predictable rendering**: Tools emit schema‑validated JSON; components render consistently across themes/devices.
- **Built for chat**: Mobile‑first, glanceable, minimal chrome; no in‑message navigation flows.
- **Developer control**: Components render; **your app** owns side effects via callbacks/server actions.
- **Type safety**: Serializable schemas on the server, parsers on the client; works great with AI SDK `InferUITools`.
- **Lifecycle aware**: Clear phases from invocation through receipt; supersession behavior is explicit.

## Mental model

See **[UI Guidelines](/docs/design-guidelines)** for the full philosophy. Quick summary:

- **5 Fundamentals:** One intent · Inline · Schema‑first · Assistant‑anchored · Lifecycle aware
- **Roles:** `information`, `decision`, `control`, `state`, `composite`
- **Lifecycle:** `invocation → output‑pending → interactive → committing → receipt → errored`
- **Conversation coherence:** Every important action ↔ one **canonical sentence**; side effects yield a **durable receipt** (status, summary, identifiers, timestamp)

## How it works

The assistant calls a tool, the tool returns JSON matching a schema, and the UI renders inline.

<Mermaid chart={`
flowchart LR
    A["Assistant calls tool"] --> B["JSON output"]
    B --> C["‹Component /›"]
    C --> D["User interacts"]
    D --> E["App handles effects"]

    style A fill:#8b5cf6,stroke:#7c3aed,stroke-width:2px,color:#fff
    style B fill:#10b981,stroke:#059669,stroke-width:2px,color:#fff
    style C fill:#3b82f6,stroke:#2563eb,stroke-width:2px,color:#fff
    style D fill:#f59e0b,stroke:#d97706,stroke-width:2px,color:#fff
    style E fill:#ef4444,stroke:#dc2626,stroke-width:2px,color:#fff

`} />

### Minimal example

**Server side:** Define a tool that returns schema-validated JSON.

**What this demonstrates:**

- `serializableMediaCardSchema` from `@tool-ui/media-card` ensures type-safe output
- Actions are defined with both UI labels and canonical sentences for conversation coherence
- The schema guarantees the frontend receives reconstructable, addressable data

```ts title="Server — define a tool with an output schema"
import { streamText, tool } from "ai";
import { openai } from "@ai-sdk/openai";
import { z } from "zod";
import { serializableMediaCardSchema } from "@/components/tool-ui/media-card/schema";

export async function POST(req: Request) {
  const { messages } = await req.json();

  const result = streamText({
    model: openai("gpt-4o"),
    messages,
    tools: {
      previewLink: tool({
        description: "Show a preview card for a URL",
        inputSchema: z.object({ url: z.string().url() }),
        outputSchema: serializableMediaCardSchema,
        async execute({ url }) {
          return {
            id: "link-preview-1",
            assetId: "link-1",
            kind: "link",
            href: url,
            title: "Example Site",
            description: "A description of the linked content",
            thumb: "https://example.com/image.jpg",
          };
        },
      }),
    },
  });

  return result.toUIMessageStreamResponse();
}
```

**Client side:** Register the component and let assistant-ui handle rendering.

**What this demonstrates:**

- `makeAssistantToolUI` connects the tool name to a React component
- The `result` object is automatically typed and validated against your schema
- Simply mounting `<PreviewLinkUI />` registers it—no manual plumbing required
- assistant-ui manages the runtime, streaming, and tool call lifecycle

```tsx title="Client — render with assistant-ui"
"use client";
import { AssistantRuntimeProvider } from "@assistant-ui/react";
import {
  useChatRuntime,
  AssistantChatTransport,
  makeAssistantToolUI,
} from "@assistant-ui/react-ai-sdk";
import { MediaCard } from "@/components/tool-ui/media-card";

const PreviewLinkUI = makeAssistantToolUI({
  toolName: "previewLink",
  render: ({ result }) => <MediaCard {...result} maxWidth="420px" />,
});

export default function App() {
  const runtime = useChatRuntime({
    transport: new AssistantChatTransport({ api: "/api/chat" }),
  });
  return (
    <AssistantRuntimeProvider runtime={runtime}>
      <PreviewLinkUI />
      {/* your <Thread /> component here */}
    </AssistantRuntimeProvider>
  );
}
```

## Anatomy of a surface

Every Tool UI surface is addressable and reconstructable. Here's the common schema structure:

**What this shows:**

- `id` makes the tool UI referenceable by the assistant ("the link preview above")
- `role` declares the primary purpose (information, decision, control, state, or composite)
- `actions` with `sentence` fields enable natural language interaction
- `receipt` provides durable proof of side effects with timestamps and identifiers

```ts
{
  id: string;                     // stable identifier for this rendering
  role: "information"|"decision"|"control"|"state"|"composite";
  actions?: Array<{ id: string; label: string; sentence: string }>;
  // optional receipt after side effects:
  receipt?: {
    outcome: "success"|"partial"|"failed"|"cancelled";
    summary: string;
    identifiers?: Record<string, string>;
    at: string;                   // ISO timestamp
  };
}
```

## Next steps

- **[Quick Start](/docs/quick-start)**: Wire your first Tool UI
- **[UI Guidelines](/docs/design-guidelines)**: Patterns, roles, lifecycle, receipts
- **[Components](/docs/data-table)**: Explore Data Table, Media Card, Option List, Social Post
