import { DocsHeader } from "../_components/docs-header";
import { Alert, AlertDescription, AlertTitle } from "@/components/ui/alert";
import { FlaskConical } from "lucide-react";

<DocsHeader
  title="Overview"
  description="A React component framework for conversation-native UIs inside AI chats."
  mdxPath="app/docs/overview/content.mdx"
/>

<Alert className="squircle my-6 flex max-w-2xl flex-col gap-3 rounded-4xl border-emerald-300 bg-emerald-50/50 p-5 dark:border-emerald-950 dark:bg-emerald-950/20">
  <AlertTitle className="mb-0! flex items-center gap-2 pb-0! font-mono text-emerald-700 dark:text-emerald-300">
    <FlaskConical className="size-4 text-emerald-600! dark:text-emerald-300!" />
    {"Research Preview"}
  </AlertTitle>
  <AlertDescription className="my-0! py-0! text-pretty text-emerald-800 dark:text-emerald-100">
    {
      "Tool UI is in active development, and APIs are still coalescing as we refine discover better patterns for conversation-native UIs."
    }
  </AlertDescription>
</Alert>

Tool UI is a React component framework for **conversation‑native** UIs inside AI chats.  
Tools return **JSON**; Tool UI renders it as **inline, narrated, referenceable** surfaces.

## At a glance

- **Conversation‑native components** that live _inside messages_, optimized for chat width and scroll.
- **Schema‑first rendering**: every surface is driven by a serializable schema with stable IDs.
- **Assistant‑anchored**: the assistant introduces, interprets, and closes each surface.
- **Stack‑agnostic**: works with any LLM/provider and orchestration layer.

## Where it fits

[Radix](https://www.radix-ui.com/)/[shadcn](https://ui.shadcn.com/) (primitives) → **Tool UI** (conversation‑native components & schema) → [AI SDK](https://ai-sdk.dev/) / [LangGraph](https://langchain-ai.github.io/langgraphjs/) / etc. (LLM orchestration)

## Who it’s for

- **React devs & design engineers** building LLM chat apps.
- Teams who want **predictable, accessible** UI that the assistant can **reference** (“the second row”).
- Anyone who wants **typed, schema‑validated** tool outputs instead of brittle HTML strings.

## Why Tool UI

- **Predictable rendering**: Tools emit schema‑validated JSON; components render consistently across themes/devices.
- **Built for chat**: Mobile‑first, glanceable, minimal chrome; no in‑message navigation flows.
- **Developer control**: Components render; **your app** owns side effects via callbacks/server actions.
- **Type safety**: Serializable schemas on the server, parsers on the client; works great with AI SDK [`InferUITools`](https://ai-sdk.dev/docs/reference/ai-sdk-ui/infer-ui-tools).
- **Lifecycle aware**: Clear phases from invocation through receipt; supersession behavior is explicit.

## Mental model

See **[UI Guidelines](/docs/design-guidelines)** for the full philosophy. Quick summary:

- **5 Fundamentals:** One intent · Inline · Schema‑first · Assistant‑anchored · Lifecycle aware
- **Roles:** `information`, `decision`, `control`, `state`, `composite`
- **Lifecycle:** `invocation → output‑pending → interactive → committing → receipt → errored`
- **Conversation coherence:** Every important action ↔ one **canonical sentence**; side effects yield a **durable receipt** (status, summary, identifiers, timestamp)

## How it works

The assistant calls a tool, the tool returns JSON matching a schema, and the UI renders inline.

<Mermaid chart={`
flowchart LR
    A["Assistant calls tool"] --> B["JSON output"]
    B --> C["‹Component /›"]
    C --> D["User interacts"]
    D --> E["App handles effects"]

    style A fill:#8b5cf6,stroke:#7c3aed,stroke-width:2px,color:#fff
    style B fill:#10b981,stroke:#059669,stroke-width:2px,color:#fff
    style C fill:#3b82f6,stroke:#2563eb,stroke-width:2px,color:#fff
    style D fill:#f59e0b,stroke:#d97706,stroke-width:2px,color:#fff
    style E fill:#ef4444,stroke:#dc2626,stroke-width:2px,color:#fff

`} />

### Minimal example

**Server side:** Define a tool that returns schema-validated JSON.

**What this demonstrates:**

- `SerializableMediaCardSchema` ensures type-safe output
- Actions are defined with both UI labels and canonical sentences for conversation coherence
- The schema guarantees the frontend receives reconstructable, addressable data

```ts title="Server: define a tool with an output schema"
import { streamText, tool, convertToModelMessages } from "ai";
import { openai } from "@ai-sdk/openai";
import { z } from "zod";
import { SerializableMediaCardSchema } from "@/components/tool-ui/media-card/schema";

export async function POST(req: Request) {
  const { messages } = await req.json();

  const result = streamText({
    model: openai("gpt-4o"),
    messages: convertToModelMessages(messages),
    tools: {
      previewLink: tool({
        description: "Show a preview card for a URL",
        inputSchema: z.object({ url: z.url() }),
        outputSchema: SerializableMediaCardSchema,
        async execute({ url }) {
          return {
            id: "link-preview-1",
            assetId: "link-1",
            kind: "link",
            href: url,
            title: "Example Site",
            description: "A description of the linked content",
            thumb: "https://example.com/image.jpg",
          };
        },
      }),
    },
  });

  return result.toUIMessageStreamResponse();
}
```

**Client side:** Register the component and let assistant-ui handle rendering.

**What this demonstrates:**

- `makeAssistantToolUI` connects the tool name to a React component
- `parseSerializableMediaCard` validates the tool result at runtime and returns typed props
- Simply mounting `<PreviewLinkUI />` registers it (no manual plumbing required)
- assistant-ui manages the runtime, streaming, and tool call lifecycle

```tsx title="Client: render with assistant-ui"
"use client";
import {
  AssistantRuntimeProvider,
  makeAssistantToolUI,
} from "@assistant-ui/react";
import {
  useChatRuntime,
  AssistantChatTransport,
} from "@assistant-ui/react-ai-sdk";
import {
  MediaCard,
  parseSerializableMediaCard,
} from "@/components/tool-ui/media-card";

const PreviewLinkUI = makeAssistantToolUI({
  toolName: "previewLink",
  render: ({ result }) => {
    const card = parseSerializableMediaCard(result);
    return <MediaCard {...card} maxWidth="420px" />;
  },
});

export default function App() {
  const runtime = useChatRuntime({
    transport: new AssistantChatTransport({ api: "/api/chat" }),
  });
  return (
    <AssistantRuntimeProvider runtime={runtime}>
      <PreviewLinkUI />
      {/* your <Thread /> component here */}
    </AssistantRuntimeProvider>
  );
}
```

## Anatomy of a surface

Every Tool UI surface is addressable and reconstructable. Here's the common schema structure:

**What this shows:**

- `id` makes the tool UI referenceable by the assistant ("the link preview above")
- `role` declares the primary purpose (information, decision, control, state, or composite)
- `actions` with `sentence` fields enable natural language interaction
- `receipt` provides durable proof of side effects with timestamps and identifiers

```ts
{
  id: string;                     // stable identifier for this rendering
  role: "information"|"decision"|"control"|"state"|"composite";
  actions?: Array<{ id: string; label: string; sentence: string }>;
  // optional receipt after side effects:
  receipt?: {
    outcome: "success"|"partial"|"failed"|"cancelled";
    summary: string;
    identifiers?: Record<string, string>;
    at: string;                   // ISO timestamp
  };
}
```

## Next steps

- **[Quick Start](/docs/quick-start)**: Wire your first Tool UI
- **[UI Guidelines](/docs/design-guidelines)**: Patterns, roles, lifecycle, receipts
- **[Components](/docs/data-table)**: Explore Data Table, Media Card, Option List, Social Post
