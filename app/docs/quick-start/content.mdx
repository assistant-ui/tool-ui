import { DocsHeader } from "../_components/docs-header";

<DocsHeader title="Quick Start" mdxPath="app/docs/quick-start/content.mdx" />

Get Tool UI working in your app in minutes. This guide uses assistant-ui (recommended) for the best experience.


## Setup

<Steps>

<Step title="Initialize assistant-ui">

Set up the assistant-ui runtime provider that manages the connection between your frontend and backend. This handles message streaming, tool execution, and state management.

**What this demonstrates:**
- `useChatRuntime` creates a runtime that connects to your backend API
- `AssistantChatTransport` handles the communication protocol with your `/api/chat` endpoint
- `AssistantRuntimeProvider` makes the runtime available to all child components

```tsx
"use client";
import { AssistantRuntimeProvider } from "@assistant-ui/react";
import { useChatRuntime, AssistantChatTransport } from "@assistant-ui/react-ai-sdk";

function App() {
  const runtime = useChatRuntime({
    transport: new AssistantChatTransport({ api: "/api/chat" }),
  });

  return (
    <AssistantRuntimeProvider runtime={runtime}>
      {/* Your chat UI */}
    </AssistantRuntimeProvider>
  );
}
```

</Step>

<Step title="Define a backend tool">

Create a tool on your server that the LLM can call. The tool returns structured JSON that matches a Tool UI component schema. Here we're creating a link preview tool that returns data for the MediaCard component.

**What this demonstrates:**
- `serializableMediaCardSchema` ensures your tool output matches the MediaCard component's expected format
- The `outputSchema` validates that returned data is type-safe and serializable
- Your tool can fetch real data (APIs, databases) and return it in the component schema format
- The LLM decides when to call this tool based on the description and user intent

```ts
import { streamText, tool } from "ai";
import { openai } from "@ai-sdk/openai";
import { z } from "zod";
// Import the schema from the Tool UI package
import { serializableMediaCardSchema } from "@tool-ui/media-card";

export async function POST(req: Request) {
  const { messages } = await req.json();

  const result = streamText({
    model: openai("gpt-4o"),
    messages,
    tools: {
      previewLink: tool({
        description: "Show a preview card for a URL",
        inputSchema: z.object({ url: z.string().url() }),
        outputSchema: serializableMediaCardSchema,
        async execute({ url }) {
          // In production, you'd fetch real metadata here
          return {
            id: "link-1",
            kind: "link",
            href: url,
            title: "Example Site",
            description: "A description of the linked content",
            thumb: "https://example.com/image.jpg",
          };
        },
      }),
    },
  });

  return result.toUIMessageStreamResponse();
}
```

</Step>

<Step title="Register the Tool UI">

Connect your backend tool to a frontend component. When the LLM calls the `previewLink` tool, assistant-ui will automatically render the MediaCard component with the returned data.

**What this demonstrates:**
- `makeAssistantToolUI` creates a component that listens for tool calls by name
- Type parameters specify the tool's input schema and output schema for full type safety
- The `toolName` must exactly match your backend tool name
- `result` contains the validated data from your backend tool's `execute` function
- Simply including `<PreviewLinkUI />` in your app registers the UI for that tool

```tsx
import { makeAssistantToolUI } from "@assistant-ui/react";
import { MediaCard, type SerializableMediaCard } from "@tool-ui/media-card";

export const PreviewLinkUI = makeAssistantToolUI<
  { url: string },
  SerializableMediaCard
>({
  toolName: "previewLink",  // Must match backend tool name
  render: ({ result }) => (
    <MediaCard {...result} maxWidth="420px" />
  ),
});

// Register in your app
function App() {
  return (
    <AssistantRuntimeProvider runtime={runtime}>
      <PreviewLinkUI />
      <Thread />
    </AssistantRuntimeProvider>
  );
}
```

That's it! When a user asks the assistant to preview a link, it will call your tool and render a beautiful MediaCard component.

</Step>

</Steps>


## Installation

The quickest way to get started is using the assistant-ui CLI, which will set up your project with all necessary dependencies:

```sh
npx assistant-ui@latest init
```

Or install packages manually if you prefer to configure yourself:

```sh
pnpm add @assistant-ui/react @assistant-ui/react-ai-sdk ai @ai-sdk/openai zod
```

Add your OpenAI API key to your environment variables:

```env
OPENAI_API_KEY=sk-...
```


## Other Frameworks

Tool UI components work with any React app. Without assistant-ui, you'll need to manually parse tool outputs and render components. For the best experience, we recommend using assistant-ui.


## Runtime Options

assistant-ui supports multiple runtimes: AI SDK, LangGraph, LangServe, Mastra, or custom backends. The examples above use AI SDK v5.


## Next Steps

- [**Explore Components**](/docs/data-table): Data Table, Media Card, Option List, Social Post
- [**UI Guidelines**](/docs/design-guidelines): Learn the interaction patterns
- [**Examples**](https://github.com/assistant-ui/assistant-ui/tree/main/examples): Full implementations
